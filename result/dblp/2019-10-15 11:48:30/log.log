Dataset: dblp
----- Opt. hyperparams -----
K: 5
batch_size: 1
nb_epochs: 50
patience: 100
lr: 0.005
l2_coef: 0.001
----- Archi. hyperparams -----
nb. layers: 1
nb. units per layer: [8]
nb. attention heads: [8, 1]
residual: False
nonlinearity: <function elu at 0x10e479a70>
model: <class 'models.gat.HeteGAT_multi'>
model: result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/1/1.ckpt
{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Fri Jul 20 22:28:51 2018', '__version__': '1.0', '__globals__': [], 'net_APTPA': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       ...,
       [0., 1., 1., ..., 0., 1., 1.],
       [0., 1., 1., ..., 1., 0., 1.],
       [0., 0., 0., ..., 1., 1., 0.]]), 'label': array([[0., 1., 0., 0.],
       [0., 0., 0., 1.],
       [1., 0., 0., 0.],
       ...,
       [0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 0., 1., 0.]]), 'train_idx': array([[   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,
          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,
          22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,
          33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,
          44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,
          55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,
          66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,
          77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,
          88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,
          99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,
         110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,
         121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,
         132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,
         143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,
         154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,
         165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,
         176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,
         187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,
         198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,
         209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,
         220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,
         231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,
         242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,
         253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,
         264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,
         275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,
         286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
         297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,
         308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,
         319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,
         330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,
         341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,
         352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,
         363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,
         374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,
         385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,
         396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,
         407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,
         418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,
         429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,
         440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,
         451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,
         462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,
         473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,
         484,  485,  486,  487,  488,  489,  490,  491,  492,  493,  494,
         495,  496,  497,  498,  499,  500,  501,  502,  503,  504,  505,
         506,  507,  508,  509,  510,  511,  512,  513,  514,  515,  516,
         517,  518,  519,  520,  521,  522,  523,  524,  525,  526,  527,
         528,  529,  530,  531,  532,  533,  534,  535,  536,  537,  538,
         539,  540,  541,  542,  543,  544,  545,  546,  547,  548,  549,
         550,  551,  552,  553,  554,  555,  556,  557,  558,  559,  560,
         561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,
         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,
         583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,
         594,  595,  596,  597,  598,  599,  600,  601,  602,  603,  604,
         605,  606,  607,  608,  609,  610,  611,  612,  613,  614,  615,
         617,  618,  619,  620,  621,  624,  625,  627,  629,  630,  633,
         634,  636,  641,  643,  644,  646,  649,  653,  658,  660,  661,
         662,  664,  674,  677,  682,  684,  685,  686,  691,  692,  693,
         699,  704,  705,  710,  711,  713,  719,  723,  725,  728,  729,
         730,  732,  734,  736,  741,  745,  748,  751,  752,  753,  754,
         757,  759,  762,  764,  766,  767,  768,  772,  775,  777,  779,
         780,  781,  784,  785,  787,  788,  790,  796,  797,  799,  804,
         806,  811,  813,  816,  818,  819,  821,  823,  825,  826,  828,
         829,  832,  833,  836,  838,  839,  844,  845,  848,  849,  851,
         855,  858,  860,  861,  862,  866,  869,  870,  872,  873,  875,
         880,  881,  884,  888,  890,  892,  894,  895,  902,  903,  905,
         925,  930,  933,  937,  943,  956,  962,  965,  967,  969,  972,
         974,  993, 1001, 1003, 1012, 1014, 1018, 1031, 1039, 1042, 1046,
        1054, 1060, 1073, 1085, 1088, 1090, 1098, 1102, 1104, 1105, 1110,
        1117, 1122, 1123, 1127, 1129, 1140, 1158, 1160, 1164, 1166, 1175,
        1177, 1186, 1190, 1191, 1194, 1206, 1212, 1219, 1220, 1222, 1223,
        1230, 1250, 1253, 1254, 1261, 1264, 1266, 1270]]), 'features': array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 1, 0, 0]]), 'net_APCPA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 1.],
       [0., 0., 0., ..., 0., 1., 1.]]), 'net_APA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 1.]]), 'test_idx': array([[ 947,  949,  952, ..., 4054, 4055, 4056]]), 'val_idx': array([[ 616,  622,  623,  626,  628,  631,  632,  635,  637,  638,  639,
         640,  642,  645,  647,  648,  650,  651,  652,  654,  655,  656,
         657,  659,  663,  665,  666,  667,  668,  669,  670,  671,  672,
         673,  675,  676,  678,  679,  680,  681,  683,  687,  688,  689,
         690,  694,  695,  696,  697,  698,  700,  701,  702,  703,  706,
         707,  708,  709,  712,  714,  715,  716,  717,  718,  720,  721,
         722,  724,  726,  727,  731,  733,  735,  737,  738,  739,  740,
         742,  743,  744,  746,  747,  749,  750,  755,  756,  758,  760,
         761,  763,  765,  769,  770,  771,  773,  774,  776,  778,  782,
         783,  786,  789,  791,  792,  793,  794,  795,  798,  800,  801,
         802,  803,  805,  807,  808,  809,  810,  812,  814,  815,  817,
         820,  822,  824,  827,  830,  831,  834,  835,  837,  840,  841,
         842,  843,  846,  847,  850,  852,  853,  854,  856,  857,  859,
         863,  864,  865,  867,  868,  871,  874,  876,  877,  878,  879,
         882,  883,  885,  886,  887,  889,  891,  893,  896,  897,  898,
         899,  900,  901,  904,  906,  907,  908,  909,  910,  911,  912,
         913,  914,  915,  916,  917,  918,  919,  920,  921,  922,  923,
         924,  926,  927,  928,  929,  931,  932,  934,  935,  936,  938,
         939,  940,  941,  942,  944,  945,  946,  948,  950,  951,  954,
         958,  959,  970,  971,  978,  980,  983,  984,  988,  989,  994,
        1002, 1008, 1010, 1013, 1015, 1019, 1020, 1021, 1022, 1029, 1033,
        1037, 1041, 1044, 1049, 1051, 1052, 1063, 1070, 1072, 1074, 1076,
        1096, 1100, 1103, 1107, 1109, 1111, 1116, 1118, 1121, 1124, 1128,
        1133, 1136, 1139, 1151, 1153, 1157, 1161, 1163, 1171, 1174, 1176,
        1178, 1182, 1195, 1198, 1199, 1201, 1203, 1205, 1208, 1209, 1210,
        1214, 1226, 1227, 1229, 1231, 1233, 1237, 1249, 1258, 1262, 1274,
        1280, 1282, 1285, 1286, 1288, 1289, 1290, 1293, 1294, 1295, 1297,
        1298, 1300, 1302, 1304, 1305, 1307, 1312, 1315, 1316, 1318, 1323,
        1325, 1328, 1331, 1334, 1336, 1344, 1348, 1349, 1354, 1357, 1359,
        1367, 1369, 1371, 1375, 1380, 1382, 1392, 1402, 1408, 1414, 1420,
        1425, 1429, 1432, 1433, 1434, 1438, 1443, 1444, 1459, 1461, 1466,
        1467, 1469, 1474, 1479, 1482, 1484, 1490, 1494, 1495, 1502, 1508,
        1516, 1532, 1533, 1540, 1541, 1544, 1547, 1556, 1558, 1559, 1565,
        1570, 1571, 1573, 1576, 1577, 1579, 1583, 1587, 1594, 1597, 1598,
        1600, 1603, 1607, 1609, 1613, 1637, 1638, 1650, 1658, 1669, 1680,
        1682, 1683, 1691, 1696, 1699, 1702, 1703, 1717, 1719, 1720, 1725,
        1748, 1751, 1758, 1759]])}
y_train:(4057, 4), y_val:(4057, 4), y_test:(4057, 4), train_idx:(1, 800), val_idx:(1, 400), test_idx:(1, 2857)
build graph...
de
Epoch: 0, att_val: [0.5340949  0.46590498]
Training: loss = 1.46410, acc = 0.22750 | Val: loss = 1.30823, acc = 0.35000
Epoch: 1, att_val: [0.66112983 0.33886996]
Training: loss = 1.34087, acc = 0.32875 | Val: loss = 1.23165, acc = 0.50000
Epoch: 2, att_val: [0.72888553 0.27111545]
Training: loss = 1.26569, acc = 0.45500 | Val: loss = 1.16049, acc = 0.67000
Epoch: 3, att_val: [0.7955742  0.20442632]
Training: loss = 1.20088, acc = 0.55750 | Val: loss = 1.09080, acc = 0.73500
Epoch: 4, att_val: [0.8422777  0.15772112]
Training: loss = 1.11606, acc = 0.67875 | Val: loss = 1.01882, acc = 0.82500
Epoch: 5, att_val: [0.86859477 0.13140579]
Training: loss = 1.04168, acc = 0.81625 | Val: loss = 0.94728, acc = 0.84750
Epoch: 6, att_val: [0.88980037 0.11020065]
Training: loss = 0.96544, acc = 0.84375 | Val: loss = 0.87572, acc = 0.86250
Epoch: 7, att_val: [0.9191994  0.08080011]
Training: loss = 0.88453, acc = 0.86875 | Val: loss = 0.80705, acc = 0.86250
Epoch: 8, att_val: [0.9266386  0.07336207]
Training: loss = 0.81245, acc = 0.87625 | Val: loss = 0.73942, acc = 0.86250
Epoch: 9, att_val: [0.9427688  0.05723063]
Training: loss = 0.76270, acc = 0.88625 | Val: loss = 0.67554, acc = 0.87000
Epoch: 10, att_val: [0.94615936 0.05384058]
Training: loss = 0.68478, acc = 0.86250 | Val: loss = 0.61220, acc = 0.87250
Epoch: 11, att_val: [0.9500878  0.04991085]
Training: loss = 0.63584, acc = 0.86250 | Val: loss = 0.55227, acc = 0.87750
Epoch: 12, att_val: [0.95718986 0.04280956]
Training: loss = 0.61295, acc = 0.87750 | Val: loss = 0.50318, acc = 0.87500
Epoch: 13, att_val: [0.95845056 0.04154793]
Training: loss = 0.54045, acc = 0.86625 | Val: loss = 0.45873, acc = 0.87750
Epoch: 14, att_val: [0.9349181  0.06508168]
Training: loss = 0.51229, acc = 0.85875 | Val: loss = 0.41470, acc = 0.87750
Epoch: 15, att_val: [0.9219227  0.07807805]
Training: loss = 0.46851, acc = 0.87500 | Val: loss = 0.37680, acc = 0.88750
Epoch: 16, att_val: [0.921751  0.0782496]
Training: loss = 0.44002, acc = 0.86625 | Val: loss = 0.35114, acc = 0.90000
Epoch: 17, att_val: [0.90276897 0.09723075]
Training: loss = 0.41159, acc = 0.87875 | Val: loss = 0.33528, acc = 0.90750
Epoch: 18, att_val: [0.9077158  0.09228311]
Training: loss = 0.39305, acc = 0.89000 | Val: loss = 0.32182, acc = 0.90750
Epoch: 19, att_val: [0.87802243 0.12197815]
Training: loss = 0.40328, acc = 0.87625 | Val: loss = 0.30650, acc = 0.91000
Epoch: 20, att_val: [0.9015203  0.09848087]
Training: loss = 0.41312, acc = 0.86750 | Val: loss = 0.28930, acc = 0.91000
Epoch: 21, att_val: [0.88310295 0.11689702]
Training: loss = 0.38626, acc = 0.87750 | Val: loss = 0.28240, acc = 0.90750
Epoch: 22, att_val: [0.8309394  0.16906142]
Training: loss = 0.38536, acc = 0.87375 | Val: loss = 0.27806, acc = 0.90750
Epoch: 23, att_val: [0.91756254 0.0824362 ]
Training: loss = 0.34986, acc = 0.87750 | Val: loss = 0.27323, acc = 0.90750
Epoch: 24, att_val: [0.8993824  0.10061835]
Training: loss = 0.32797, acc = 0.88625 | Val: loss = 0.26913, acc = 0.90500
Epoch: 25, att_val: [0.9252005 0.0748004]
Training: loss = 0.35516, acc = 0.87750 | Val: loss = 0.26654, acc = 0.90500
Epoch: 26, att_val: [0.9115177  0.08848219]
Training: loss = 0.32990, acc = 0.89000 | Val: loss = 0.26706, acc = 0.90250
Epoch: 27, att_val: [0.87994444 0.12005558]
Training: loss = 0.30635, acc = 0.89875 | Val: loss = 0.26693, acc = 0.90250
Epoch: 28, att_val: [0.87890935 0.12109069]
Training: loss = 0.30727, acc = 0.90375 | Val: loss = 0.26610, acc = 0.90250
Epoch: 29, att_val: [0.9056702 0.0943305]
Training: loss = 0.31055, acc = 0.89250 | Val: loss = 0.26355, acc = 0.90250
Epoch: 30, att_val: [0.8863536  0.11364619]
Training: loss = 0.32181, acc = 0.87750 | Val: loss = 0.26024, acc = 0.90250
Epoch: 31, att_val: [0.8545398  0.14545959]
Training: loss = 0.29920, acc = 0.89750 | Val: loss = 0.25652, acc = 0.91000
Epoch: 32, att_val: [0.8516672  0.14833231]
Training: loss = 0.27562, acc = 0.90875 | Val: loss = 0.25193, acc = 0.91750
Epoch: 33, att_val: [0.86714685 0.13285303]
Training: loss = 0.28361, acc = 0.90875 | Val: loss = 0.24755, acc = 0.92000
Epoch: 34, att_val: [0.8140579 0.1859417]
Training: loss = 0.29046, acc = 0.90250 | Val: loss = 0.24528, acc = 0.92000
Epoch: 35, att_val: [0.86059475 0.13940537]
Training: loss = 0.28992, acc = 0.90125 | Val: loss = 0.24298, acc = 0.92000
Epoch: 36, att_val: [0.8542256  0.14577472]
Training: loss = 0.26647, acc = 0.91750 | Val: loss = 0.24136, acc = 0.91750
Epoch: 37, att_val: [0.82591933 0.17408139]
Training: loss = 0.26843, acc = 0.90750 | Val: loss = 0.23911, acc = 0.92000
Epoch: 38, att_val: [0.85652626 0.14347446]
Training: loss = 0.26346, acc = 0.91375 | Val: loss = 0.23775, acc = 0.92250
Epoch: 39, att_val: [0.8562535  0.14374751]
Training: loss = 0.26952, acc = 0.90750 | Val: loss = 0.23703, acc = 0.92250
Epoch: 40, att_val: [0.84089565 0.15910447]
Training: loss = 0.25797, acc = 0.92250 | Val: loss = 0.23522, acc = 0.92500
Epoch: 41, att_val: [0.8470502 0.1529476]
Training: loss = 0.25661, acc = 0.90625 | Val: loss = 0.23446, acc = 0.92500
Epoch: 42, att_val: [0.81165665 0.18834327]
Training: loss = 0.24589, acc = 0.91375 | Val: loss = 0.23373, acc = 0.92500
Epoch: 43, att_val: [0.82135504 0.17864554]
Training: loss = 0.25697, acc = 0.90375 | Val: loss = 0.23245, acc = 0.92500
Epoch: 44, att_val: [0.83168286 0.16831629]
Training: loss = 0.24424, acc = 0.91875 | Val: loss = 0.23120, acc = 0.93000
Epoch: 45, att_val: [0.82013535 0.17986414]
Training: loss = 0.24227, acc = 0.92500 | Val: loss = 0.22989, acc = 0.93000
Epoch: 46, att_val: [0.78370404 0.216296  ]
Training: loss = 0.24311, acc = 0.91625 | Val: loss = 0.22887, acc = 0.93000
Epoch: 47, att_val: [0.7980291  0.20197223]
Training: loss = 0.24547, acc = 0.92750 | Val: loss = 0.22815, acc = 0.93000
Epoch: 48, att_val: [0.77984256 0.2201592 ]
Training: loss = 0.22279, acc = 0.92500 | Val: loss = 0.22804, acc = 0.93250
Epoch: 49, att_val: [0.7376216  0.26237887]
Training: loss = 0.22868, acc = 0.92500 | Val: loss = 0.23085, acc = 0.92250
load model from : result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/1/1.ckpt
jhy_final_embedding (4057, 64) 
 [[-0.21899503  0.09599337  0.31057063 ...  0.7271542  -0.24917057
  -0.2908083 ]
 [-0.06232473 -0.12554795 -0.0971773  ... -0.10048217  0.09090969
  -0.28371608]
 [-0.15583739  0.21767795  0.31570852 ... -0.1359276   0.5470059
  -0.39750212]
 ...
 [-0.19788742 -0.028738   -0.06310582 ... -0.04938387  0.19382648
  -0.18498594]
 [-0.24879658 -0.01519453 -0.16726902 ...  0.0161918   0.15025139
  -0.111431  ]
 [-0.32224646  0.06901734 -0.19150957 ...  0.23016629  0.22133195
   0.01324644]]
Test loss: 0.23062129318714142 ; Test accuracy: 0.9310429692268372
start knn, kmean.....
xx: (2857, 64), yy: (2857, 4)
KNN(10avg, split:0.2, k=5) f1_macro: 0.9207, f1_micro: 0.9297
KNN(10avg, split:0.4, k=5) f1_macro: 0.9214, f1_micro: 0.9300
KNN(10avg, split:0.6, k=5) f1_macro: 0.9251, f1_micro: 0.9339
KNN(10avg, split:0.8, k=5) f1_macro: 0.9257, f1_micro: 0.9355
NMI (10 avg): 0.7735 , ARI (10avg): 0.8276
model: result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/2/2.ckpt
{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Fri Jul 20 22:28:51 2018', '__version__': '1.0', '__globals__': [], 'net_APTPA': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       ...,
       [0., 1., 1., ..., 0., 1., 1.],
       [0., 1., 1., ..., 1., 0., 1.],
       [0., 0., 0., ..., 1., 1., 0.]]), 'label': array([[0., 1., 0., 0.],
       [0., 0., 0., 1.],
       [1., 0., 0., 0.],
       ...,
       [0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 0., 1., 0.]]), 'train_idx': array([[   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,
          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,
          22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,
          33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,
          44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,
          55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,
          66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,
          77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,
          88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,
          99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,
         110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,
         121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,
         132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,
         143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,
         154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,
         165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,
         176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,
         187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,
         198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,
         209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,
         220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,
         231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,
         242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,
         253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,
         264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,
         275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,
         286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
         297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,
         308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,
         319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,
         330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,
         341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,
         352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,
         363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,
         374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,
         385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,
         396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,
         407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,
         418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,
         429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,
         440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,
         451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,
         462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,
         473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,
         484,  485,  486,  487,  488,  489,  490,  491,  492,  493,  494,
         495,  496,  497,  498,  499,  500,  501,  502,  503,  504,  505,
         506,  507,  508,  509,  510,  511,  512,  513,  514,  515,  516,
         517,  518,  519,  520,  521,  522,  523,  524,  525,  526,  527,
         528,  529,  530,  531,  532,  533,  534,  535,  536,  537,  538,
         539,  540,  541,  542,  543,  544,  545,  546,  547,  548,  549,
         550,  551,  552,  553,  554,  555,  556,  557,  558,  559,  560,
         561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,
         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,
         583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,
         594,  595,  596,  597,  598,  599,  600,  601,  602,  603,  604,
         605,  606,  607,  608,  609,  610,  611,  612,  613,  614,  615,
         617,  618,  619,  620,  621,  624,  625,  627,  629,  630,  633,
         634,  636,  641,  643,  644,  646,  649,  653,  658,  660,  661,
         662,  664,  674,  677,  682,  684,  685,  686,  691,  692,  693,
         699,  704,  705,  710,  711,  713,  719,  723,  725,  728,  729,
         730,  732,  734,  736,  741,  745,  748,  751,  752,  753,  754,
         757,  759,  762,  764,  766,  767,  768,  772,  775,  777,  779,
         780,  781,  784,  785,  787,  788,  790,  796,  797,  799,  804,
         806,  811,  813,  816,  818,  819,  821,  823,  825,  826,  828,
         829,  832,  833,  836,  838,  839,  844,  845,  848,  849,  851,
         855,  858,  860,  861,  862,  866,  869,  870,  872,  873,  875,
         880,  881,  884,  888,  890,  892,  894,  895,  902,  903,  905,
         925,  930,  933,  937,  943,  956,  962,  965,  967,  969,  972,
         974,  993, 1001, 1003, 1012, 1014, 1018, 1031, 1039, 1042, 1046,
        1054, 1060, 1073, 1085, 1088, 1090, 1098, 1102, 1104, 1105, 1110,
        1117, 1122, 1123, 1127, 1129, 1140, 1158, 1160, 1164, 1166, 1175,
        1177, 1186, 1190, 1191, 1194, 1206, 1212, 1219, 1220, 1222, 1223,
        1230, 1250, 1253, 1254, 1261, 1264, 1266, 1270]]), 'features': array([[ 0.        ,  0.        ,  0.        , ...,  0.7271542 ,
        -0.24917057, -0.29080829],
       [ 0.        ,  0.        ,  0.        , ..., -0.10048217,
         0.09090969, -0.28371608],
       [ 0.        ,  0.        ,  0.        , ..., -0.1359276 ,
         0.54700589, -0.39750212],
       ...,
       [ 0.        ,  0.        ,  0.        , ..., -0.04938387,
         0.19382648, -0.18498594],
       [ 0.        ,  0.        ,  0.        , ...,  0.0161918 ,
         0.15025139, -0.111431  ],
       [ 0.        ,  0.        ,  0.        , ...,  0.23016629,
         0.22133195,  0.01324644]]), 'net_APCPA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 1.],
       [0., 0., 0., ..., 0., 1., 1.]]), 'net_APA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 1.]]), 'test_idx': array([[ 947,  949,  952, ..., 4054, 4055, 4056]]), 'val_idx': array([[ 616,  622,  623,  626,  628,  631,  632,  635,  637,  638,  639,
         640,  642,  645,  647,  648,  650,  651,  652,  654,  655,  656,
         657,  659,  663,  665,  666,  667,  668,  669,  670,  671,  672,
         673,  675,  676,  678,  679,  680,  681,  683,  687,  688,  689,
         690,  694,  695,  696,  697,  698,  700,  701,  702,  703,  706,
         707,  708,  709,  712,  714,  715,  716,  717,  718,  720,  721,
         722,  724,  726,  727,  731,  733,  735,  737,  738,  739,  740,
         742,  743,  744,  746,  747,  749,  750,  755,  756,  758,  760,
         761,  763,  765,  769,  770,  771,  773,  774,  776,  778,  782,
         783,  786,  789,  791,  792,  793,  794,  795,  798,  800,  801,
         802,  803,  805,  807,  808,  809,  810,  812,  814,  815,  817,
         820,  822,  824,  827,  830,  831,  834,  835,  837,  840,  841,
         842,  843,  846,  847,  850,  852,  853,  854,  856,  857,  859,
         863,  864,  865,  867,  868,  871,  874,  876,  877,  878,  879,
         882,  883,  885,  886,  887,  889,  891,  893,  896,  897,  898,
         899,  900,  901,  904,  906,  907,  908,  909,  910,  911,  912,
         913,  914,  915,  916,  917,  918,  919,  920,  921,  922,  923,
         924,  926,  927,  928,  929,  931,  932,  934,  935,  936,  938,
         939,  940,  941,  942,  944,  945,  946,  948,  950,  951,  954,
         958,  959,  970,  971,  978,  980,  983,  984,  988,  989,  994,
        1002, 1008, 1010, 1013, 1015, 1019, 1020, 1021, 1022, 1029, 1033,
        1037, 1041, 1044, 1049, 1051, 1052, 1063, 1070, 1072, 1074, 1076,
        1096, 1100, 1103, 1107, 1109, 1111, 1116, 1118, 1121, 1124, 1128,
        1133, 1136, 1139, 1151, 1153, 1157, 1161, 1163, 1171, 1174, 1176,
        1178, 1182, 1195, 1198, 1199, 1201, 1203, 1205, 1208, 1209, 1210,
        1214, 1226, 1227, 1229, 1231, 1233, 1237, 1249, 1258, 1262, 1274,
        1280, 1282, 1285, 1286, 1288, 1289, 1290, 1293, 1294, 1295, 1297,
        1298, 1300, 1302, 1304, 1305, 1307, 1312, 1315, 1316, 1318, 1323,
        1325, 1328, 1331, 1334, 1336, 1344, 1348, 1349, 1354, 1357, 1359,
        1367, 1369, 1371, 1375, 1380, 1382, 1392, 1402, 1408, 1414, 1420,
        1425, 1429, 1432, 1433, 1434, 1438, 1443, 1444, 1459, 1461, 1466,
        1467, 1469, 1474, 1479, 1482, 1484, 1490, 1494, 1495, 1502, 1508,
        1516, 1532, 1533, 1540, 1541, 1544, 1547, 1556, 1558, 1559, 1565,
        1570, 1571, 1573, 1576, 1577, 1579, 1583, 1587, 1594, 1597, 1598,
        1600, 1603, 1607, 1609, 1613, 1637, 1638, 1650, 1658, 1669, 1680,
        1682, 1683, 1691, 1696, 1699, 1702, 1703, 1717, 1719, 1720, 1725,
        1748, 1751, 1758, 1759]])}
y_train:(4057, 4), y_val:(4057, 4), y_test:(4057, 4), train_idx:(1, 800), val_idx:(1, 400), test_idx:(1, 2857)
build graph...
de
Epoch: 0, att_val: [0.47389275 0.5261071 ]
Training: loss = 1.53395, acc = 0.20625 | Val: loss = 1.12768, acc = 0.75500
Epoch: 1, att_val: [0.6680735 0.3319274]
Training: loss = 1.19790, acc = 0.44250 | Val: loss = 0.89570, acc = 0.88500
Epoch: 2, att_val: [0.76595336 0.23404723]
Training: loss = 0.96873, acc = 0.73875 | Val: loss = 0.71648, acc = 0.89500
Epoch: 3, att_val: [0.80426747 0.19573195]
Training: loss = 0.78125, acc = 0.85000 | Val: loss = 0.57737, acc = 0.90750
Epoch: 4, att_val: [0.8295511  0.17044954]
Training: loss = 0.63729, acc = 0.88250 | Val: loss = 0.47425, acc = 0.89500
Epoch: 5, att_val: [0.8537214  0.14627819]
Training: loss = 0.52893, acc = 0.89250 | Val: loss = 0.40108, acc = 0.89000
Epoch: 6, att_val: [0.8682278  0.13177264]
Training: loss = 0.45651, acc = 0.87750 | Val: loss = 0.34825, acc = 0.89750
Epoch: 7, att_val: [0.86444086 0.13555953]
Training: loss = 0.40416, acc = 0.89625 | Val: loss = 0.31295, acc = 0.90500
Epoch: 8, att_val: [0.8872372  0.11276215]
Training: loss = 0.35560, acc = 0.90125 | Val: loss = 0.28787, acc = 0.90500
Epoch: 9, att_val: [0.8883477  0.11165161]
Training: loss = 0.32302, acc = 0.90125 | Val: loss = 0.26912, acc = 0.90750
Epoch: 10, att_val: [0.8930097  0.10699026]
Training: loss = 0.30672, acc = 0.90500 | Val: loss = 0.25432, acc = 0.90750
Epoch: 11, att_val: [0.88249797 0.11750159]
Training: loss = 0.29599, acc = 0.90250 | Val: loss = 0.24284, acc = 0.90750
Epoch: 12, att_val: [0.8803035  0.11969609]
Training: loss = 0.29517, acc = 0.90125 | Val: loss = 0.23460, acc = 0.91000
Epoch: 13, att_val: [0.8658164  0.13418366]
Training: loss = 0.28577, acc = 0.90875 | Val: loss = 0.22855, acc = 0.92000
Epoch: 14, att_val: [0.85108554 0.14891312]
Training: loss = 0.26325, acc = 0.91500 | Val: loss = 0.22433, acc = 0.91750
Epoch: 15, att_val: [0.8240818  0.17591894]
Training: loss = 0.26784, acc = 0.91750 | Val: loss = 0.22265, acc = 0.91750
Epoch: 16, att_val: [0.8045317  0.19546889]
Training: loss = 0.28108, acc = 0.90000 | Val: loss = 0.22140, acc = 0.91250
Epoch: 17, att_val: [0.80015296 0.19984812]
Training: loss = 0.25969, acc = 0.91000 | Val: loss = 0.22018, acc = 0.91500
Epoch: 18, att_val: [0.7880509 0.2119479]
Training: loss = 0.26784, acc = 0.90500 | Val: loss = 0.21989, acc = 0.91750
Epoch: 19, att_val: [0.77005863 0.2299416 ]
Training: loss = 0.25827, acc = 0.90875 | Val: loss = 0.22079, acc = 0.91250
Epoch: 20, att_val: [0.77805126 0.22194925]
Training: loss = 0.26145, acc = 0.91375 | Val: loss = 0.22170, acc = 0.91000
Epoch: 21, att_val: [0.7629102 0.2370896]
Training: loss = 0.24365, acc = 0.90750 | Val: loss = 0.22268, acc = 0.91000
Epoch: 22, att_val: [0.73860633 0.2613936 ]
Training: loss = 0.23263, acc = 0.91750 | Val: loss = 0.22333, acc = 0.91750
Epoch: 23, att_val: [0.72598094 0.27401924]
Training: loss = 0.23507, acc = 0.91625 | Val: loss = 0.22334, acc = 0.91750
Epoch: 24, att_val: [0.705848  0.2941506]
Training: loss = 0.23701, acc = 0.91750 | Val: loss = 0.22287, acc = 0.92250
Epoch: 25, att_val: [0.6823244  0.31767437]
Training: loss = 0.23354, acc = 0.91750 | Val: loss = 0.22101, acc = 0.92250
Epoch: 26, att_val: [0.64451486 0.3554861 ]
Training: loss = 0.21952, acc = 0.91375 | Val: loss = 0.21662, acc = 0.92250
Epoch: 27, att_val: [0.6306999  0.36929953]
Training: loss = 0.23359, acc = 0.91750 | Val: loss = 0.21175, acc = 0.92250
Epoch: 28, att_val: [0.65827817 0.34172192]
Training: loss = 0.21011, acc = 0.91875 | Val: loss = 0.20776, acc = 0.92750
Epoch: 29, att_val: [0.6647621  0.33523747]
Training: loss = 0.20758, acc = 0.92375 | Val: loss = 0.20520, acc = 0.92750
Epoch: 30, att_val: [0.6644373  0.33556187]
Training: loss = 0.20723, acc = 0.92375 | Val: loss = 0.20424, acc = 0.92500
Epoch: 31, att_val: [0.6376703  0.36233056]
Training: loss = 0.19920, acc = 0.92875 | Val: loss = 0.20383, acc = 0.92500
Epoch: 32, att_val: [0.6482358 0.3517646]
Training: loss = 0.19762, acc = 0.93125 | Val: loss = 0.20303, acc = 0.92750
Epoch: 33, att_val: [0.6327296  0.36727074]
Training: loss = 0.18747, acc = 0.92375 | Val: loss = 0.20096, acc = 0.93000
Epoch: 34, att_val: [0.6487025  0.35129663]
Training: loss = 0.19954, acc = 0.92625 | Val: loss = 0.20026, acc = 0.93000
Epoch: 35, att_val: [0.6556656  0.34433377]
Training: loss = 0.20658, acc = 0.93375 | Val: loss = 0.20109, acc = 0.93000
Epoch: 36, att_val: [0.67137134 0.328629  ]
Training: loss = 0.20599, acc = 0.92500 | Val: loss = 0.20165, acc = 0.93250
Epoch: 37, att_val: [0.66621155 0.3337876 ]
Training: loss = 0.16977, acc = 0.94625 | Val: loss = 0.20277, acc = 0.93750
Epoch: 38, att_val: [0.6491694  0.35083142]
Training: loss = 0.18512, acc = 0.93875 | Val: loss = 0.20297, acc = 0.93750
Epoch: 39, att_val: [0.6538338 0.3461671]
Training: loss = 0.18173, acc = 0.93250 | Val: loss = 0.20217, acc = 0.94000
Epoch: 40, att_val: [0.6592867 0.3407132]
Training: loss = 0.17971, acc = 0.93125 | Val: loss = 0.20122, acc = 0.93250
Epoch: 41, att_val: [0.6462422  0.35375762]
Training: loss = 0.17894, acc = 0.94250 | Val: loss = 0.19962, acc = 0.93250
Epoch: 42, att_val: [0.62786454 0.37213647]
Training: loss = 0.15413, acc = 0.94375 | Val: loss = 0.19613, acc = 0.92750
Epoch: 43, att_val: [0.64506125 0.35493886]
Training: loss = 0.17453, acc = 0.93750 | Val: loss = 0.19300, acc = 0.92750
Epoch: 44, att_val: [0.62906325 0.37093708]
Training: loss = 0.16085, acc = 0.94750 | Val: loss = 0.19269, acc = 0.92500
Epoch: 45, att_val: [0.6179936  0.38200593]
Training: loss = 0.16165, acc = 0.94625 | Val: loss = 0.19306, acc = 0.92250
Epoch: 46, att_val: [0.6510272  0.34897324]
Training: loss = 0.17565, acc = 0.93875 | Val: loss = 0.19268, acc = 0.92500
Epoch: 47, att_val: [0.60459614 0.39540264]
Training: loss = 0.15730, acc = 0.94625 | Val: loss = 0.19393, acc = 0.92750
Epoch: 48, att_val: [0.5959674  0.40403277]
Training: loss = 0.14245, acc = 0.95500 | Val: loss = 0.19765, acc = 0.92750
Epoch: 49, att_val: [0.56203324 0.4379672 ]
Training: loss = 0.15210, acc = 0.94625 | Val: loss = 0.20210, acc = 0.92250
load model from : result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/2/2.ckpt
jhy_final_embedding (4057, 64) 
 [[-0.20559177 -0.31317002  0.2508571  ...  0.6403892   1.5663825
  -0.35076332]
 [ 0.64027023  0.45107645  0.26041615 ... -0.6313773   0.375207
   0.55376464]
 [ 0.981523    0.66164565 -0.30559242 ...  0.69306755 -0.36419272
   1.1561855 ]
 ...
 [ 0.2177197   0.45241386  0.08048183 ... -0.55905217  0.3296964
   0.50054944]
 [ 0.2380869   0.45542312  0.26318473 ... -0.5009879   0.15568514
   0.42233378]
 [-0.5604466  -0.01330797  0.26896805 ...  0.4791535   0.7830819
  -0.48168945]]
Test loss: 0.16944538056850433 ; Test accuracy: 0.9394433498382568
start knn, kmean.....
xx: (2857, 64), yy: (2857, 4)
KNN(10avg, split:0.2, k=5) f1_macro: 0.9276, f1_micro: 0.9361
KNN(10avg, split:0.4, k=5) f1_macro: 0.9302, f1_micro: 0.9381
KNN(10avg, split:0.6, k=5) f1_macro: 0.9268, f1_micro: 0.9342
KNN(10avg, split:0.8, k=5) f1_macro: 0.9291, f1_micro: 0.9364
NMI (10 avg): 0.7938 , ARI (10avg): 0.8509
model: result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/3/3.ckpt
{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Fri Jul 20 22:28:51 2018', '__version__': '1.0', '__globals__': [], 'net_APTPA': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       ...,
       [0., 1., 1., ..., 0., 1., 1.],
       [0., 1., 1., ..., 1., 0., 1.],
       [0., 0., 0., ..., 1., 1., 0.]]), 'label': array([[0., 1., 0., 0.],
       [0., 0., 0., 1.],
       [1., 0., 0., 0.],
       ...,
       [0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 0., 1., 0.]]), 'train_idx': array([[   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,
          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,
          22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,
          33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,
          44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,
          55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,
          66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,
          77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,
          88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,
          99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,
         110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,
         121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,
         132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,
         143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,
         154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,
         165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,
         176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,
         187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,
         198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,
         209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,
         220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,
         231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,
         242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,
         253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,
         264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,
         275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,
         286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
         297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,
         308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,
         319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,
         330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,
         341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,
         352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,
         363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,
         374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,
         385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,
         396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,
         407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,
         418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,
         429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,
         440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,
         451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,
         462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,
         473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,
         484,  485,  486,  487,  488,  489,  490,  491,  492,  493,  494,
         495,  496,  497,  498,  499,  500,  501,  502,  503,  504,  505,
         506,  507,  508,  509,  510,  511,  512,  513,  514,  515,  516,
         517,  518,  519,  520,  521,  522,  523,  524,  525,  526,  527,
         528,  529,  530,  531,  532,  533,  534,  535,  536,  537,  538,
         539,  540,  541,  542,  543,  544,  545,  546,  547,  548,  549,
         550,  551,  552,  553,  554,  555,  556,  557,  558,  559,  560,
         561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,
         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,
         583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,
         594,  595,  596,  597,  598,  599,  600,  601,  602,  603,  604,
         605,  606,  607,  608,  609,  610,  611,  612,  613,  614,  615,
         617,  618,  619,  620,  621,  624,  625,  627,  629,  630,  633,
         634,  636,  641,  643,  644,  646,  649,  653,  658,  660,  661,
         662,  664,  674,  677,  682,  684,  685,  686,  691,  692,  693,
         699,  704,  705,  710,  711,  713,  719,  723,  725,  728,  729,
         730,  732,  734,  736,  741,  745,  748,  751,  752,  753,  754,
         757,  759,  762,  764,  766,  767,  768,  772,  775,  777,  779,
         780,  781,  784,  785,  787,  788,  790,  796,  797,  799,  804,
         806,  811,  813,  816,  818,  819,  821,  823,  825,  826,  828,
         829,  832,  833,  836,  838,  839,  844,  845,  848,  849,  851,
         855,  858,  860,  861,  862,  866,  869,  870,  872,  873,  875,
         880,  881,  884,  888,  890,  892,  894,  895,  902,  903,  905,
         925,  930,  933,  937,  943,  956,  962,  965,  967,  969,  972,
         974,  993, 1001, 1003, 1012, 1014, 1018, 1031, 1039, 1042, 1046,
        1054, 1060, 1073, 1085, 1088, 1090, 1098, 1102, 1104, 1105, 1110,
        1117, 1122, 1123, 1127, 1129, 1140, 1158, 1160, 1164, 1166, 1175,
        1177, 1186, 1190, 1191, 1194, 1206, 1212, 1219, 1220, 1222, 1223,
        1230, 1250, 1253, 1254, 1261, 1264, 1266, 1270]]), 'features': array([[ 0.        ,  0.        ,  0.        , ...,  0.6403892 ,
         1.56638253, -0.35076332],
       [ 0.        ,  0.        ,  0.        , ..., -0.63137728,
         0.37520701,  0.55376464],
       [ 0.        ,  0.        ,  0.        , ...,  0.69306755,
        -0.36419272,  1.15618551],
       ...,
       [ 0.        ,  0.        ,  0.        , ..., -0.55905217,
         0.32969639,  0.50054944],
       [ 0.        ,  0.        ,  0.        , ..., -0.50098789,
         0.15568514,  0.42233378],
       [ 0.        ,  0.        ,  0.        , ...,  0.47915351,
         0.78308189, -0.48168945]]), 'net_APCPA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 1.],
       [0., 0., 0., ..., 0., 1., 1.]]), 'net_APA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 1.]]), 'test_idx': array([[ 947,  949,  952, ..., 4054, 4055, 4056]]), 'val_idx': array([[ 616,  622,  623,  626,  628,  631,  632,  635,  637,  638,  639,
         640,  642,  645,  647,  648,  650,  651,  652,  654,  655,  656,
         657,  659,  663,  665,  666,  667,  668,  669,  670,  671,  672,
         673,  675,  676,  678,  679,  680,  681,  683,  687,  688,  689,
         690,  694,  695,  696,  697,  698,  700,  701,  702,  703,  706,
         707,  708,  709,  712,  714,  715,  716,  717,  718,  720,  721,
         722,  724,  726,  727,  731,  733,  735,  737,  738,  739,  740,
         742,  743,  744,  746,  747,  749,  750,  755,  756,  758,  760,
         761,  763,  765,  769,  770,  771,  773,  774,  776,  778,  782,
         783,  786,  789,  791,  792,  793,  794,  795,  798,  800,  801,
         802,  803,  805,  807,  808,  809,  810,  812,  814,  815,  817,
         820,  822,  824,  827,  830,  831,  834,  835,  837,  840,  841,
         842,  843,  846,  847,  850,  852,  853,  854,  856,  857,  859,
         863,  864,  865,  867,  868,  871,  874,  876,  877,  878,  879,
         882,  883,  885,  886,  887,  889,  891,  893,  896,  897,  898,
         899,  900,  901,  904,  906,  907,  908,  909,  910,  911,  912,
         913,  914,  915,  916,  917,  918,  919,  920,  921,  922,  923,
         924,  926,  927,  928,  929,  931,  932,  934,  935,  936,  938,
         939,  940,  941,  942,  944,  945,  946,  948,  950,  951,  954,
         958,  959,  970,  971,  978,  980,  983,  984,  988,  989,  994,
        1002, 1008, 1010, 1013, 1015, 1019, 1020, 1021, 1022, 1029, 1033,
        1037, 1041, 1044, 1049, 1051, 1052, 1063, 1070, 1072, 1074, 1076,
        1096, 1100, 1103, 1107, 1109, 1111, 1116, 1118, 1121, 1124, 1128,
        1133, 1136, 1139, 1151, 1153, 1157, 1161, 1163, 1171, 1174, 1176,
        1178, 1182, 1195, 1198, 1199, 1201, 1203, 1205, 1208, 1209, 1210,
        1214, 1226, 1227, 1229, 1231, 1233, 1237, 1249, 1258, 1262, 1274,
        1280, 1282, 1285, 1286, 1288, 1289, 1290, 1293, 1294, 1295, 1297,
        1298, 1300, 1302, 1304, 1305, 1307, 1312, 1315, 1316, 1318, 1323,
        1325, 1328, 1331, 1334, 1336, 1344, 1348, 1349, 1354, 1357, 1359,
        1367, 1369, 1371, 1375, 1380, 1382, 1392, 1402, 1408, 1414, 1420,
        1425, 1429, 1432, 1433, 1434, 1438, 1443, 1444, 1459, 1461, 1466,
        1467, 1469, 1474, 1479, 1482, 1484, 1490, 1494, 1495, 1502, 1508,
        1516, 1532, 1533, 1540, 1541, 1544, 1547, 1556, 1558, 1559, 1565,
        1570, 1571, 1573, 1576, 1577, 1579, 1583, 1587, 1594, 1597, 1598,
        1600, 1603, 1607, 1609, 1613, 1637, 1638, 1650, 1658, 1669, 1680,
        1682, 1683, 1691, 1696, 1699, 1702, 1703, 1717, 1719, 1720, 1725,
        1748, 1751, 1758, 1759]])}
y_train:(4057, 4), y_val:(4057, 4), y_test:(4057, 4), train_idx:(1, 800), val_idx:(1, 400), test_idx:(1, 2857)
build graph...
de
Epoch: 0, att_val: [0.5398311 0.4601682]
Training: loss = 1.53279, acc = 0.21375 | Val: loss = 0.83883, acc = 0.89000
Epoch: 1, att_val: [0.7214472 0.278554 ]
Training: loss = 0.93247, acc = 0.73875 | Val: loss = 0.51341, acc = 0.90250
Epoch: 2, att_val: [0.7988888 0.201112 ]
Training: loss = 0.59296, acc = 0.85625 | Val: loss = 0.36103, acc = 0.91250
Epoch: 3, att_val: [0.82109207 0.17890936]
Training: loss = 0.43380, acc = 0.88375 | Val: loss = 0.30794, acc = 0.90250
Epoch: 4, att_val: [0.8287505 0.1712492]
Training: loss = 0.35509, acc = 0.89125 | Val: loss = 0.28149, acc = 0.91000
Epoch: 5, att_val: [0.83680266 0.16319785]
Training: loss = 0.32191, acc = 0.89375 | Val: loss = 0.25910, acc = 0.91000
Epoch: 6, att_val: [0.8420071  0.15799193]
Training: loss = 0.30341, acc = 0.89250 | Val: loss = 0.24257, acc = 0.90750
Epoch: 7, att_val: [0.82821566 0.17178528]
Training: loss = 0.28957, acc = 0.89750 | Val: loss = 0.22956, acc = 0.90750
Epoch: 8, att_val: [0.81141514 0.18858418]
Training: loss = 0.30254, acc = 0.90125 | Val: loss = 0.22127, acc = 0.92000
Epoch: 9, att_val: [0.81004333 0.18995675]
Training: loss = 0.26692, acc = 0.91875 | Val: loss = 0.21515, acc = 0.92250
Epoch: 10, att_val: [0.7840944  0.21590509]
Training: loss = 0.29239, acc = 0.90125 | Val: loss = 0.21282, acc = 0.91750
Epoch: 11, att_val: [0.7512811  0.24871826]
Training: loss = 0.26812, acc = 0.90750 | Val: loss = 0.21562, acc = 0.91250
Epoch: 12, att_val: [0.6207851 0.3792145]
Training: loss = 0.25759, acc = 0.90750 | Val: loss = 0.22591, acc = 0.91500
Epoch: 13, att_val: [0.66489345 0.33510703]
Training: loss = 0.24921, acc = 0.90875 | Val: loss = 0.23183, acc = 0.91750
Epoch: 14, att_val: [0.637987   0.36201346]
Training: loss = 0.26165, acc = 0.90750 | Val: loss = 0.23255, acc = 0.91750
Epoch: 15, att_val: [0.70257497 0.29742506]
Training: loss = 0.24307, acc = 0.91000 | Val: loss = 0.22833, acc = 0.91750
Epoch: 16, att_val: [0.6580651  0.34193522]
Training: loss = 0.25481, acc = 0.91125 | Val: loss = 0.22306, acc = 0.92750
Epoch: 17, att_val: [0.70367754 0.29632252]
Training: loss = 0.24923, acc = 0.90375 | Val: loss = 0.21630, acc = 0.93000
Epoch: 18, att_val: [0.68546927 0.31452972]
Training: loss = 0.22595, acc = 0.91500 | Val: loss = 0.21244, acc = 0.93000
Epoch: 19, att_val: [0.6739259  0.32607582]
Training: loss = 0.24681, acc = 0.91625 | Val: loss = 0.21242, acc = 0.92750
Epoch: 20, att_val: [0.6986155 0.3013851]
Training: loss = 0.21462, acc = 0.92375 | Val: loss = 0.21538, acc = 0.93000
Epoch: 21, att_val: [0.71587807 0.28412175]
Training: loss = 0.23438, acc = 0.92500 | Val: loss = 0.22506, acc = 0.92750
Epoch: 22, att_val: [0.73633873 0.26366177]
Training: loss = 0.21647, acc = 0.92375 | Val: loss = 0.24263, acc = 0.92500
Epoch: 23, att_val: [0.7147598  0.28523904]
Training: loss = 0.22173, acc = 0.91875 | Val: loss = 0.24073, acc = 0.92500
Epoch: 24, att_val: [0.6598574 0.3401432]
Training: loss = 0.21864, acc = 0.92125 | Val: loss = 0.23543, acc = 0.92000
Epoch: 25, att_val: [0.64870095 0.35129905]
Training: loss = 0.21121, acc = 0.92500 | Val: loss = 0.22891, acc = 0.92000
Epoch: 26, att_val: [0.67614    0.32385972]
Training: loss = 0.20261, acc = 0.92125 | Val: loss = 0.22200, acc = 0.92500
Epoch: 27, att_val: [0.659987   0.34001255]
Training: loss = 0.21422, acc = 0.92375 | Val: loss = 0.21679, acc = 0.92750
Epoch: 28, att_val: [0.6457965  0.35420376]
Training: loss = 0.19945, acc = 0.93125 | Val: loss = 0.21461, acc = 0.92750
Epoch: 29, att_val: [0.6812283  0.31877166]
Training: loss = 0.18758, acc = 0.94500 | Val: loss = 0.21278, acc = 0.92750
Epoch: 30, att_val: [0.67568    0.32432035]
Training: loss = 0.19106, acc = 0.93250 | Val: loss = 0.21324, acc = 0.92500
Epoch: 31, att_val: [0.64765644 0.35234436]
Training: loss = 0.20076, acc = 0.93500 | Val: loss = 0.21562, acc = 0.92750
Epoch: 32, att_val: [0.65219307 0.34780538]
Training: loss = 0.18673, acc = 0.93500 | Val: loss = 0.22067, acc = 0.92750
Epoch: 33, att_val: [0.6572294  0.34277117]
Training: loss = 0.18816, acc = 0.93875 | Val: loss = 0.22544, acc = 0.92500
Epoch: 34, att_val: [0.6431006  0.35689905]
Training: loss = 0.18853, acc = 0.93500 | Val: loss = 0.22905, acc = 0.92750
Epoch: 35, att_val: [0.6182428  0.38175827]
Training: loss = 0.18988, acc = 0.92625 | Val: loss = 0.22535, acc = 0.92250
Epoch: 36, att_val: [0.6461875  0.35381138]
Training: loss = 0.18972, acc = 0.93625 | Val: loss = 0.22097, acc = 0.92750
Epoch: 37, att_val: [0.6631369 0.3368629]
Training: loss = 0.17076, acc = 0.94000 | Val: loss = 0.21635, acc = 0.93000
Epoch: 38, att_val: [0.683522   0.31647804]
Training: loss = 0.18484, acc = 0.94375 | Val: loss = 0.21067, acc = 0.92500
Epoch: 39, att_val: [0.67818856 0.3218113 ]
Training: loss = 0.17113, acc = 0.94500 | Val: loss = 0.20972, acc = 0.92750
Epoch: 40, att_val: [0.67428297 0.3257157 ]
Training: loss = 0.16388, acc = 0.93750 | Val: loss = 0.20849, acc = 0.93000
Epoch: 41, att_val: [0.659215  0.3407858]
Training: loss = 0.16666, acc = 0.94000 | Val: loss = 0.20956, acc = 0.93250
Epoch: 42, att_val: [0.65441006 0.34559023]
Training: loss = 0.16245, acc = 0.94750 | Val: loss = 0.20987, acc = 0.93500
Epoch: 43, att_val: [0.6560358  0.34396368]
Training: loss = 0.17119, acc = 0.94250 | Val: loss = 0.21233, acc = 0.93500
Epoch: 44, att_val: [0.6409924  0.35900825]
Training: loss = 0.17705, acc = 0.94500 | Val: loss = 0.21330, acc = 0.93250
Epoch: 45, att_val: [0.64057046 0.35943002]
Training: loss = 0.15779, acc = 0.94625 | Val: loss = 0.21058, acc = 0.93250
Epoch: 46, att_val: [0.6200905  0.37990916]
Training: loss = 0.17134, acc = 0.94750 | Val: loss = 0.20853, acc = 0.92750
Epoch: 47, att_val: [0.6241651  0.37583464]
Training: loss = 0.16365, acc = 0.94750 | Val: loss = 0.20698, acc = 0.92750
Epoch: 48, att_val: [0.6351365  0.36486384]
Training: loss = 0.15442, acc = 0.95000 | Val: loss = 0.20594, acc = 0.92750
Epoch: 49, att_val: [0.66296667 0.33703303]
Training: loss = 0.16810, acc = 0.93625 | Val: loss = 0.20605, acc = 0.92750
load model from : result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/3/3.ckpt
jhy_final_embedding (4057, 64) 
 [[ 0.9967908  -0.86264676  1.2901431  ... -0.7964233  -0.1904229
  -0.74238676]
 [-0.35812804 -0.52147937 -0.4315162  ...  0.5969646   0.98016405
  -0.20288429]
 [-0.07877937  0.84245026 -0.65429807 ... -0.53165877 -0.6852434
   0.21666633]
 ...
 [-0.29156792 -0.4718476  -0.34307507 ...  0.52877915  0.70809543
  -0.0223551 ]
 [ 0.14218606 -0.4877255   0.25281554 ...  0.30286878  0.9316361
  -0.36410624]
 [ 0.9149514  -0.15797254  2.1467714  ... -0.6697434   1.0622061
  -0.7230524 ]]
Test loss: 0.1729615181684494 ; Test accuracy: 0.941893458366394
start knn, kmean.....
xx: (2857, 64), yy: (2857, 4)
KNN(10avg, split:0.2, k=5) f1_macro: 0.9271, f1_micro: 0.9355
KNN(10avg, split:0.4, k=5) f1_macro: 0.9276, f1_micro: 0.9358
KNN(10avg, split:0.6, k=5) f1_macro: 0.9272, f1_micro: 0.9353
KNN(10avg, split:0.8, k=5) f1_macro: 0.9256, f1_micro: 0.9343
NMI (10 avg): 0.7733 , ARI (10avg): 0.8147
model: result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/4/4.ckpt
{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Fri Jul 20 22:28:51 2018', '__version__': '1.0', '__globals__': [], 'net_APTPA': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       ...,
       [0., 1., 1., ..., 0., 1., 1.],
       [0., 1., 1., ..., 1., 0., 1.],
       [0., 0., 0., ..., 1., 1., 0.]]), 'label': array([[0., 1., 0., 0.],
       [0., 0., 0., 1.],
       [1., 0., 0., 0.],
       ...,
       [0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 0., 1., 0.]]), 'train_idx': array([[   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,
          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,
          22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,
          33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,
          44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,
          55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,
          66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,
          77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,
          88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,
          99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,
         110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,
         121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,
         132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,
         143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,
         154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,
         165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,
         176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,
         187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,
         198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,
         209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,
         220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,
         231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,
         242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,
         253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,
         264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,
         275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,
         286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
         297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,
         308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,
         319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,
         330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,
         341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,
         352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,
         363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,
         374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,
         385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,
         396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,
         407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,
         418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,
         429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,
         440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,
         451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,
         462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,
         473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,
         484,  485,  486,  487,  488,  489,  490,  491,  492,  493,  494,
         495,  496,  497,  498,  499,  500,  501,  502,  503,  504,  505,
         506,  507,  508,  509,  510,  511,  512,  513,  514,  515,  516,
         517,  518,  519,  520,  521,  522,  523,  524,  525,  526,  527,
         528,  529,  530,  531,  532,  533,  534,  535,  536,  537,  538,
         539,  540,  541,  542,  543,  544,  545,  546,  547,  548,  549,
         550,  551,  552,  553,  554,  555,  556,  557,  558,  559,  560,
         561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,
         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,
         583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,
         594,  595,  596,  597,  598,  599,  600,  601,  602,  603,  604,
         605,  606,  607,  608,  609,  610,  611,  612,  613,  614,  615,
         617,  618,  619,  620,  621,  624,  625,  627,  629,  630,  633,
         634,  636,  641,  643,  644,  646,  649,  653,  658,  660,  661,
         662,  664,  674,  677,  682,  684,  685,  686,  691,  692,  693,
         699,  704,  705,  710,  711,  713,  719,  723,  725,  728,  729,
         730,  732,  734,  736,  741,  745,  748,  751,  752,  753,  754,
         757,  759,  762,  764,  766,  767,  768,  772,  775,  777,  779,
         780,  781,  784,  785,  787,  788,  790,  796,  797,  799,  804,
         806,  811,  813,  816,  818,  819,  821,  823,  825,  826,  828,
         829,  832,  833,  836,  838,  839,  844,  845,  848,  849,  851,
         855,  858,  860,  861,  862,  866,  869,  870,  872,  873,  875,
         880,  881,  884,  888,  890,  892,  894,  895,  902,  903,  905,
         925,  930,  933,  937,  943,  956,  962,  965,  967,  969,  972,
         974,  993, 1001, 1003, 1012, 1014, 1018, 1031, 1039, 1042, 1046,
        1054, 1060, 1073, 1085, 1088, 1090, 1098, 1102, 1104, 1105, 1110,
        1117, 1122, 1123, 1127, 1129, 1140, 1158, 1160, 1164, 1166, 1175,
        1177, 1186, 1190, 1191, 1194, 1206, 1212, 1219, 1220, 1222, 1223,
        1230, 1250, 1253, 1254, 1261, 1264, 1266, 1270]]), 'features': array([[ 0.        ,  0.        ,  0.        , ..., -0.79642332,
        -0.19042289, -0.74238676],
       [ 0.        ,  0.        ,  0.        , ...,  0.5969646 ,
         0.98016405, -0.20288429],
       [ 0.        ,  0.        ,  0.        , ..., -0.53165877,
        -0.68524343,  0.21666633],
       ...,
       [ 0.        ,  0.        ,  0.        , ...,  0.52877915,
         0.70809543, -0.0223551 ],
       [ 0.        ,  0.        ,  0.        , ...,  0.30286878,
         0.9316361 , -0.36410624],
       [ 0.        ,  0.        ,  0.        , ..., -0.66974342,
         1.06220615, -0.72305238]]), 'net_APCPA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 1.],
       [0., 0., 0., ..., 0., 1., 1.]]), 'net_APA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 1.]]), 'test_idx': array([[ 947,  949,  952, ..., 4054, 4055, 4056]]), 'val_idx': array([[ 616,  622,  623,  626,  628,  631,  632,  635,  637,  638,  639,
         640,  642,  645,  647,  648,  650,  651,  652,  654,  655,  656,
         657,  659,  663,  665,  666,  667,  668,  669,  670,  671,  672,
         673,  675,  676,  678,  679,  680,  681,  683,  687,  688,  689,
         690,  694,  695,  696,  697,  698,  700,  701,  702,  703,  706,
         707,  708,  709,  712,  714,  715,  716,  717,  718,  720,  721,
         722,  724,  726,  727,  731,  733,  735,  737,  738,  739,  740,
         742,  743,  744,  746,  747,  749,  750,  755,  756,  758,  760,
         761,  763,  765,  769,  770,  771,  773,  774,  776,  778,  782,
         783,  786,  789,  791,  792,  793,  794,  795,  798,  800,  801,
         802,  803,  805,  807,  808,  809,  810,  812,  814,  815,  817,
         820,  822,  824,  827,  830,  831,  834,  835,  837,  840,  841,
         842,  843,  846,  847,  850,  852,  853,  854,  856,  857,  859,
         863,  864,  865,  867,  868,  871,  874,  876,  877,  878,  879,
         882,  883,  885,  886,  887,  889,  891,  893,  896,  897,  898,
         899,  900,  901,  904,  906,  907,  908,  909,  910,  911,  912,
         913,  914,  915,  916,  917,  918,  919,  920,  921,  922,  923,
         924,  926,  927,  928,  929,  931,  932,  934,  935,  936,  938,
         939,  940,  941,  942,  944,  945,  946,  948,  950,  951,  954,
         958,  959,  970,  971,  978,  980,  983,  984,  988,  989,  994,
        1002, 1008, 1010, 1013, 1015, 1019, 1020, 1021, 1022, 1029, 1033,
        1037, 1041, 1044, 1049, 1051, 1052, 1063, 1070, 1072, 1074, 1076,
        1096, 1100, 1103, 1107, 1109, 1111, 1116, 1118, 1121, 1124, 1128,
        1133, 1136, 1139, 1151, 1153, 1157, 1161, 1163, 1171, 1174, 1176,
        1178, 1182, 1195, 1198, 1199, 1201, 1203, 1205, 1208, 1209, 1210,
        1214, 1226, 1227, 1229, 1231, 1233, 1237, 1249, 1258, 1262, 1274,
        1280, 1282, 1285, 1286, 1288, 1289, 1290, 1293, 1294, 1295, 1297,
        1298, 1300, 1302, 1304, 1305, 1307, 1312, 1315, 1316, 1318, 1323,
        1325, 1328, 1331, 1334, 1336, 1344, 1348, 1349, 1354, 1357, 1359,
        1367, 1369, 1371, 1375, 1380, 1382, 1392, 1402, 1408, 1414, 1420,
        1425, 1429, 1432, 1433, 1434, 1438, 1443, 1444, 1459, 1461, 1466,
        1467, 1469, 1474, 1479, 1482, 1484, 1490, 1494, 1495, 1502, 1508,
        1516, 1532, 1533, 1540, 1541, 1544, 1547, 1556, 1558, 1559, 1565,
        1570, 1571, 1573, 1576, 1577, 1579, 1583, 1587, 1594, 1597, 1598,
        1600, 1603, 1607, 1609, 1613, 1637, 1638, 1650, 1658, 1669, 1680,
        1682, 1683, 1691, 1696, 1699, 1702, 1703, 1717, 1719, 1720, 1725,
        1748, 1751, 1758, 1759]])}
y_train:(4057, 4), y_val:(4057, 4), y_test:(4057, 4), train_idx:(1, 800), val_idx:(1, 400), test_idx:(1, 2857)
build graph...
de
Epoch: 0, att_val: [0.47781688 0.52218264]
Training: loss = 1.83250, acc = 0.23625 | Val: loss = 0.56339, acc = 0.88250
Epoch: 1, att_val: [0.7004285  0.29957095]
Training: loss = 0.70980, acc = 0.79375 | Val: loss = 0.33680, acc = 0.89000
Epoch: 2, att_val: [0.7969805  0.20302054]
Training: loss = 0.40063, acc = 0.86875 | Val: loss = 0.26697, acc = 0.91250
Epoch: 3, att_val: [0.8356937  0.16430537]
Training: loss = 0.31632, acc = 0.88500 | Val: loss = 0.24350, acc = 0.92000
Epoch: 4, att_val: [0.8573578  0.14264183]
Training: loss = 0.31537, acc = 0.87875 | Val: loss = 0.23817, acc = 0.91750
Epoch: 5, att_val: [0.8438178  0.15618132]
Training: loss = 0.31640, acc = 0.88750 | Val: loss = 0.23865, acc = 0.91500
Epoch: 6, att_val: [0.8313084  0.16869217]
Training: loss = 0.32346, acc = 0.88750 | Val: loss = 0.23265, acc = 0.92000
Epoch: 7, att_val: [0.79045045 0.20954938]
Training: loss = 0.30605, acc = 0.90125 | Val: loss = 0.23329, acc = 0.92750
Epoch: 8, att_val: [0.664387 0.335613]
Training: loss = 0.28496, acc = 0.90375 | Val: loss = 0.23953, acc = 0.93500
Epoch: 9, att_val: [0.6449178  0.35508266]
Training: loss = 0.29825, acc = 0.90500 | Val: loss = 0.24492, acc = 0.93250
Epoch: 10, att_val: [0.61526704 0.3847332 ]
Training: loss = 0.28879, acc = 0.91250 | Val: loss = 0.25049, acc = 0.93250
Epoch: 11, att_val: [0.63749593 0.36250353]
Training: loss = 0.28824, acc = 0.91000 | Val: loss = 0.25646, acc = 0.93000
Epoch: 12, att_val: [0.6359477  0.36405367]
Training: loss = 0.26787, acc = 0.90625 | Val: loss = 0.26298, acc = 0.92750
Epoch: 13, att_val: [0.6483059  0.35169423]
Training: loss = 0.28161, acc = 0.91000 | Val: loss = 0.26521, acc = 0.92500
Epoch: 14, att_val: [0.654774   0.34522608]
Training: loss = 0.26958, acc = 0.91375 | Val: loss = 0.26146, acc = 0.92000
Epoch: 15, att_val: [0.66787684 0.33212426]
Training: loss = 0.26065, acc = 0.92125 | Val: loss = 0.25417, acc = 0.92250
Epoch: 16, att_val: [0.6507529  0.34924752]
Training: loss = 0.24386, acc = 0.91500 | Val: loss = 0.24823, acc = 0.92500
Epoch: 17, att_val: [0.6577208  0.34227884]
Training: loss = 0.24274, acc = 0.91500 | Val: loss = 0.24390, acc = 0.92250
Epoch: 18, att_val: [0.6624925  0.33750635]
Training: loss = 0.26539, acc = 0.91375 | Val: loss = 0.24174, acc = 0.92000
Epoch: 19, att_val: [0.6647634 0.3352371]
Training: loss = 0.24170, acc = 0.92000 | Val: loss = 0.24350, acc = 0.92000
Epoch: 20, att_val: [0.6747059  0.32529414]
Training: loss = 0.23161, acc = 0.91750 | Val: loss = 0.24372, acc = 0.91750
Epoch: 21, att_val: [0.66614527 0.333855  ]
Training: loss = 0.22469, acc = 0.92500 | Val: loss = 0.24309, acc = 0.92000
Epoch: 22, att_val: [0.64817387 0.3518265 ]
Training: loss = 0.22852, acc = 0.91750 | Val: loss = 0.24171, acc = 0.92500
Epoch: 23, att_val: [0.65602446 0.34397423]
Training: loss = 0.21303, acc = 0.93250 | Val: loss = 0.23778, acc = 0.92500
Epoch: 24, att_val: [0.6492436  0.35075584]
Training: loss = 0.20292, acc = 0.92375 | Val: loss = 0.23270, acc = 0.92250
Epoch: 25, att_val: [0.6297635  0.37023702]
Training: loss = 0.22080, acc = 0.93250 | Val: loss = 0.22876, acc = 0.92250
Epoch: 26, att_val: [0.65589905 0.34410134]
Training: loss = 0.21215, acc = 0.93375 | Val: loss = 0.22765, acc = 0.92250
Epoch: 27, att_val: [0.65325874 0.3467415 ]
Training: loss = 0.20038, acc = 0.93125 | Val: loss = 0.22721, acc = 0.92250
Epoch: 28, att_val: [0.6368512  0.36314964]
Training: loss = 0.19687, acc = 0.92875 | Val: loss = 0.22580, acc = 0.92250
Epoch: 29, att_val: [0.6533919  0.34660846]
Training: loss = 0.19809, acc = 0.92750 | Val: loss = 0.22501, acc = 0.92250
Epoch: 30, att_val: [0.65188885 0.34811056]
Training: loss = 0.20716, acc = 0.92375 | Val: loss = 0.22470, acc = 0.92750
Epoch: 31, att_val: [0.66642356 0.3335757 ]
Training: loss = 0.18934, acc = 0.92375 | Val: loss = 0.22569, acc = 0.92500
Epoch: 32, att_val: [0.64830464 0.35169572]
Training: loss = 0.19750, acc = 0.93000 | Val: loss = 0.22749, acc = 0.92000
Epoch: 33, att_val: [0.636247   0.36375237]
Training: loss = 0.20659, acc = 0.92625 | Val: loss = 0.22896, acc = 0.92000
Epoch: 34, att_val: [0.604177   0.39582324]
Training: loss = 0.19341, acc = 0.93250 | Val: loss = 0.22967, acc = 0.93000
Epoch: 35, att_val: [0.59719694 0.40280366]
Training: loss = 0.18418, acc = 0.93750 | Val: loss = 0.22994, acc = 0.92750
Epoch: 36, att_val: [0.6282895  0.37171102]
Training: loss = 0.18793, acc = 0.93750 | Val: loss = 0.22937, acc = 0.92750
Epoch: 37, att_val: [0.62284327 0.37715587]
Training: loss = 0.18522, acc = 0.94375 | Val: loss = 0.22786, acc = 0.92500
Epoch: 38, att_val: [0.62050974 0.3794888 ]
Training: loss = 0.17263, acc = 0.93625 | Val: loss = 0.22689, acc = 0.92500
Epoch: 39, att_val: [0.670565   0.32943398]
Training: loss = 0.18129, acc = 0.93375 | Val: loss = 0.22383, acc = 0.92750
Epoch: 40, att_val: [0.6806788 0.3193226]
Training: loss = 0.17494, acc = 0.93750 | Val: loss = 0.22094, acc = 0.92750
Epoch: 41, att_val: [0.69985086 0.30014947]
Training: loss = 0.17638, acc = 0.93625 | Val: loss = 0.22099, acc = 0.92750
Epoch: 42, att_val: [0.7050486  0.29495168]
Training: loss = 0.19630, acc = 0.92750 | Val: loss = 0.22539, acc = 0.92500
Epoch: 43, att_val: [0.7005911  0.29940835]
Training: loss = 0.16034, acc = 0.94625 | Val: loss = 0.22894, acc = 0.92250
Epoch: 44, att_val: [0.70229256 0.2977089 ]
Training: loss = 0.15879, acc = 0.94250 | Val: loss = 0.23356, acc = 0.92250
Epoch: 45, att_val: [0.70238125 0.29761884]
Training: loss = 0.17039, acc = 0.94625 | Val: loss = 0.23068, acc = 0.92250
Epoch: 46, att_val: [0.69066316 0.3093363 ]
Training: loss = 0.16345, acc = 0.94875 | Val: loss = 0.22720, acc = 0.92500
Epoch: 47, att_val: [0.6966665  0.30333397]
Training: loss = 0.16861, acc = 0.94875 | Val: loss = 0.22435, acc = 0.92500
Epoch: 48, att_val: [0.68822765 0.3117734 ]
Training: loss = 0.17359, acc = 0.94500 | Val: loss = 0.22415, acc = 0.92500
Epoch: 49, att_val: [0.6863765 0.3136226]
Training: loss = 0.16271, acc = 0.95375 | Val: loss = 0.22420, acc = 0.92500
load model from : result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/4/4.ckpt
jhy_final_embedding (4057, 64) 
 [[ 0.15430191  0.303715   -0.679446   ...  0.9734615  -0.16021
   1.016351  ]
 [-0.32202482 -0.24712026  0.61128813 ...  0.10559439 -0.6026567
  -0.45885876]
 [-0.69067234 -0.6820224  -0.4752452  ... -0.7486439   2.462528
   2.511711  ]
 ...
 [-0.2591114  -0.26886263  0.53734374 ...  0.00663745 -0.5860476
  -0.42735535]
 [ 0.07981142 -0.14900568  0.27036104 ...  0.63943654 -0.7008553
  -0.567101  ]
 [ 1.2750463   0.58468026 -0.46883836 ...  1.2983563  -0.8413956
  -0.70342016]]
Test loss: 0.23111948370933533 ; Test accuracy: 0.9124920964241028
start knn, kmean.....
xx: (2857, 64), yy: (2857, 4)
KNN(10avg, split:0.2, k=5) f1_macro: 0.9202, f1_micro: 0.9294
KNN(10avg, split:0.4, k=5) f1_macro: 0.9251, f1_micro: 0.9329
KNN(10avg, split:0.6, k=5) f1_macro: 0.9241, f1_micro: 0.9325
KNN(10avg, split:0.8, k=5) f1_macro: 0.9241, f1_micro: 0.9327
NMI (10 avg): 0.7493 , ARI (10avg): 0.8061
model: result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/5/5.ckpt
{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Fri Jul 20 22:28:51 2018', '__version__': '1.0', '__globals__': [], 'net_APTPA': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       ...,
       [0., 1., 1., ..., 0., 1., 1.],
       [0., 1., 1., ..., 1., 0., 1.],
       [0., 0., 0., ..., 1., 1., 0.]]), 'label': array([[0., 1., 0., 0.],
       [0., 0., 0., 1.],
       [1., 0., 0., 0.],
       ...,
       [0., 0., 0., 1.],
       [0., 0., 0., 1.],
       [0., 0., 1., 0.]]), 'train_idx': array([[   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,
          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,
          22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,
          33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,
          44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,
          55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,
          66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,
          77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,
          88,   89,   90,   91,   92,   93,   94,   95,   96,   97,   98,
          99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,
         110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,
         121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,
         132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,
         143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,
         154,  155,  156,  157,  158,  159,  160,  161,  162,  163,  164,
         165,  166,  167,  168,  169,  170,  171,  172,  173,  174,  175,
         176,  177,  178,  179,  180,  181,  182,  183,  184,  185,  186,
         187,  188,  189,  190,  191,  192,  193,  194,  195,  196,  197,
         198,  199,  200,  201,  202,  203,  204,  205,  206,  207,  208,
         209,  210,  211,  212,  213,  214,  215,  216,  217,  218,  219,
         220,  221,  222,  223,  224,  225,  226,  227,  228,  229,  230,
         231,  232,  233,  234,  235,  236,  237,  238,  239,  240,  241,
         242,  243,  244,  245,  246,  247,  248,  249,  250,  251,  252,
         253,  254,  255,  256,  257,  258,  259,  260,  261,  262,  263,
         264,  265,  266,  267,  268,  269,  270,  271,  272,  273,  274,
         275,  276,  277,  278,  279,  280,  281,  282,  283,  284,  285,
         286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,
         297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  307,
         308,  309,  310,  311,  312,  313,  314,  315,  316,  317,  318,
         319,  320,  321,  322,  323,  324,  325,  326,  327,  328,  329,
         330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,
         341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,
         352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,
         363,  364,  365,  366,  367,  368,  369,  370,  371,  372,  373,
         374,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,
         385,  386,  387,  388,  389,  390,  391,  392,  393,  394,  395,
         396,  397,  398,  399,  400,  401,  402,  403,  404,  405,  406,
         407,  408,  409,  410,  411,  412,  413,  414,  415,  416,  417,
         418,  419,  420,  421,  422,  423,  424,  425,  426,  427,  428,
         429,  430,  431,  432,  433,  434,  435,  436,  437,  438,  439,
         440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,
         451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,
         462,  463,  464,  465,  466,  467,  468,  469,  470,  471,  472,
         473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,
         484,  485,  486,  487,  488,  489,  490,  491,  492,  493,  494,
         495,  496,  497,  498,  499,  500,  501,  502,  503,  504,  505,
         506,  507,  508,  509,  510,  511,  512,  513,  514,  515,  516,
         517,  518,  519,  520,  521,  522,  523,  524,  525,  526,  527,
         528,  529,  530,  531,  532,  533,  534,  535,  536,  537,  538,
         539,  540,  541,  542,  543,  544,  545,  546,  547,  548,  549,
         550,  551,  552,  553,  554,  555,  556,  557,  558,  559,  560,
         561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,
         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,
         583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,
         594,  595,  596,  597,  598,  599,  600,  601,  602,  603,  604,
         605,  606,  607,  608,  609,  610,  611,  612,  613,  614,  615,
         617,  618,  619,  620,  621,  624,  625,  627,  629,  630,  633,
         634,  636,  641,  643,  644,  646,  649,  653,  658,  660,  661,
         662,  664,  674,  677,  682,  684,  685,  686,  691,  692,  693,
         699,  704,  705,  710,  711,  713,  719,  723,  725,  728,  729,
         730,  732,  734,  736,  741,  745,  748,  751,  752,  753,  754,
         757,  759,  762,  764,  766,  767,  768,  772,  775,  777,  779,
         780,  781,  784,  785,  787,  788,  790,  796,  797,  799,  804,
         806,  811,  813,  816,  818,  819,  821,  823,  825,  826,  828,
         829,  832,  833,  836,  838,  839,  844,  845,  848,  849,  851,
         855,  858,  860,  861,  862,  866,  869,  870,  872,  873,  875,
         880,  881,  884,  888,  890,  892,  894,  895,  902,  903,  905,
         925,  930,  933,  937,  943,  956,  962,  965,  967,  969,  972,
         974,  993, 1001, 1003, 1012, 1014, 1018, 1031, 1039, 1042, 1046,
        1054, 1060, 1073, 1085, 1088, 1090, 1098, 1102, 1104, 1105, 1110,
        1117, 1122, 1123, 1127, 1129, 1140, 1158, 1160, 1164, 1166, 1175,
        1177, 1186, 1190, 1191, 1194, 1206, 1212, 1219, 1220, 1222, 1223,
        1230, 1250, 1253, 1254, 1261, 1264, 1266, 1270]]), 'features': array([[ 0.        ,  0.        ,  0.        , ...,  0.97346151,
        -0.16021   ,  1.01635098],
       [ 0.        ,  0.        ,  0.        , ...,  0.10559439,
        -0.60265672, -0.45885876],
       [ 0.        ,  0.        ,  0.        , ..., -0.74864388,
         2.46252799,  2.51171088],
       ...,
       [ 0.        ,  0.        ,  0.        , ...,  0.00663745,
        -0.58604759, -0.42735535],
       [ 0.        ,  0.        ,  0.        , ...,  0.63943654,
        -0.70085531, -0.567101  ],
       [ 0.        ,  0.        ,  0.        , ...,  1.29835629,
        -0.84139562, -0.70342016]]), 'net_APCPA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 0., ..., 1., 1., 0.],
       [0., 1., 0., ..., 1., 1., 1.],
       [0., 0., 0., ..., 0., 1., 1.]]), 'net_APA': array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 1.]]), 'test_idx': array([[ 947,  949,  952, ..., 4054, 4055, 4056]]), 'val_idx': array([[ 616,  622,  623,  626,  628,  631,  632,  635,  637,  638,  639,
         640,  642,  645,  647,  648,  650,  651,  652,  654,  655,  656,
         657,  659,  663,  665,  666,  667,  668,  669,  670,  671,  672,
         673,  675,  676,  678,  679,  680,  681,  683,  687,  688,  689,
         690,  694,  695,  696,  697,  698,  700,  701,  702,  703,  706,
         707,  708,  709,  712,  714,  715,  716,  717,  718,  720,  721,
         722,  724,  726,  727,  731,  733,  735,  737,  738,  739,  740,
         742,  743,  744,  746,  747,  749,  750,  755,  756,  758,  760,
         761,  763,  765,  769,  770,  771,  773,  774,  776,  778,  782,
         783,  786,  789,  791,  792,  793,  794,  795,  798,  800,  801,
         802,  803,  805,  807,  808,  809,  810,  812,  814,  815,  817,
         820,  822,  824,  827,  830,  831,  834,  835,  837,  840,  841,
         842,  843,  846,  847,  850,  852,  853,  854,  856,  857,  859,
         863,  864,  865,  867,  868,  871,  874,  876,  877,  878,  879,
         882,  883,  885,  886,  887,  889,  891,  893,  896,  897,  898,
         899,  900,  901,  904,  906,  907,  908,  909,  910,  911,  912,
         913,  914,  915,  916,  917,  918,  919,  920,  921,  922,  923,
         924,  926,  927,  928,  929,  931,  932,  934,  935,  936,  938,
         939,  940,  941,  942,  944,  945,  946,  948,  950,  951,  954,
         958,  959,  970,  971,  978,  980,  983,  984,  988,  989,  994,
        1002, 1008, 1010, 1013, 1015, 1019, 1020, 1021, 1022, 1029, 1033,
        1037, 1041, 1044, 1049, 1051, 1052, 1063, 1070, 1072, 1074, 1076,
        1096, 1100, 1103, 1107, 1109, 1111, 1116, 1118, 1121, 1124, 1128,
        1133, 1136, 1139, 1151, 1153, 1157, 1161, 1163, 1171, 1174, 1176,
        1178, 1182, 1195, 1198, 1199, 1201, 1203, 1205, 1208, 1209, 1210,
        1214, 1226, 1227, 1229, 1231, 1233, 1237, 1249, 1258, 1262, 1274,
        1280, 1282, 1285, 1286, 1288, 1289, 1290, 1293, 1294, 1295, 1297,
        1298, 1300, 1302, 1304, 1305, 1307, 1312, 1315, 1316, 1318, 1323,
        1325, 1328, 1331, 1334, 1336, 1344, 1348, 1349, 1354, 1357, 1359,
        1367, 1369, 1371, 1375, 1380, 1382, 1392, 1402, 1408, 1414, 1420,
        1425, 1429, 1432, 1433, 1434, 1438, 1443, 1444, 1459, 1461, 1466,
        1467, 1469, 1474, 1479, 1482, 1484, 1490, 1494, 1495, 1502, 1508,
        1516, 1532, 1533, 1540, 1541, 1544, 1547, 1556, 1558, 1559, 1565,
        1570, 1571, 1573, 1576, 1577, 1579, 1583, 1587, 1594, 1597, 1598,
        1600, 1603, 1607, 1609, 1613, 1637, 1638, 1650, 1658, 1669, 1680,
        1682, 1683, 1691, 1696, 1699, 1702, 1703, 1717, 1719, 1720, 1725,
        1748, 1751, 1758, 1759]])}
y_train:(4057, 4), y_val:(4057, 4), y_test:(4057, 4), train_idx:(1, 800), val_idx:(1, 400), test_idx:(1, 2857)
build graph...
de
Epoch: 0, att_val: [0.4554599  0.54454046]
Training: loss = 1.80105, acc = 0.22500 | Val: loss = 0.51562, acc = 0.87750
Epoch: 1, att_val: [0.75481313 0.24518749]
Training: loss = 0.63942, acc = 0.79875 | Val: loss = 0.27911, acc = 0.91250
Epoch: 2, att_val: [0.8275541  0.17244639]
Training: loss = 0.35037, acc = 0.87875 | Val: loss = 0.25158, acc = 0.89500
Epoch: 3, att_val: [0.8658373  0.13416235]
Training: loss = 0.32544, acc = 0.88000 | Val: loss = 0.25373, acc = 0.90250
Epoch: 4, att_val: [0.8801598  0.11983993]
Training: loss = 0.33012, acc = 0.88125 | Val: loss = 0.24712, acc = 0.91500
Epoch: 5, att_val: [0.8606701  0.13933058]
Training: loss = 0.32355, acc = 0.89250 | Val: loss = 0.24102, acc = 0.92000
Epoch: 6, att_val: [0.8519114  0.14808801]
Training: loss = 0.34213, acc = 0.89000 | Val: loss = 0.23944, acc = 0.92000
Epoch: 7, att_val: [0.82191026 0.17808984]
Training: loss = 0.30377, acc = 0.90750 | Val: loss = 0.23783, acc = 0.91750
Epoch: 8, att_val: [0.78884417 0.21115655]
Training: loss = 0.31107, acc = 0.90250 | Val: loss = 0.24442, acc = 0.92250
Epoch: 9, att_val: [0.7545772 0.2454223]
Training: loss = 0.29387, acc = 0.91000 | Val: loss = 0.24880, acc = 0.92500
Epoch: 10, att_val: [0.6961173  0.30388206]
Training: loss = 0.28540, acc = 0.90875 | Val: loss = 0.24767, acc = 0.92750
Epoch: 11, att_val: [0.6467324 0.353267 ]
Training: loss = 0.25520, acc = 0.92000 | Val: loss = 0.24602, acc = 0.92750
Epoch: 12, att_val: [0.6436726  0.35632762]
Training: loss = 0.27578, acc = 0.91500 | Val: loss = 0.24211, acc = 0.93000
Epoch: 13, att_val: [0.6340306 0.3659694]
Training: loss = 0.28022, acc = 0.90375 | Val: loss = 0.24159, acc = 0.92250
Epoch: 14, att_val: [0.65707046 0.34292892]
Training: loss = 0.24856, acc = 0.92125 | Val: loss = 0.24827, acc = 0.91500
Epoch: 15, att_val: [0.69441354 0.30558702]
Training: loss = 0.30514, acc = 0.90875 | Val: loss = 0.24769, acc = 0.91750
Epoch: 16, att_val: [0.7001029  0.29989654]
Training: loss = 0.25848, acc = 0.90875 | Val: loss = 0.24443, acc = 0.92000
Epoch: 17, att_val: [0.7363619  0.26363945]
Training: loss = 0.25225, acc = 0.90500 | Val: loss = 0.23739, acc = 0.92250
Epoch: 18, att_val: [0.74942553 0.2505736 ]
Training: loss = 0.24127, acc = 0.91875 | Val: loss = 0.24107, acc = 0.91500
Epoch: 19, att_val: [0.762011   0.23798946]
Training: loss = 0.23845, acc = 0.92250 | Val: loss = 0.24349, acc = 0.92000
Epoch: 20, att_val: [0.77755463 0.22244444]
Training: loss = 0.24698, acc = 0.92000 | Val: loss = 0.24825, acc = 0.92500
Epoch: 21, att_val: [0.766944   0.23305544]
Training: loss = 0.23409, acc = 0.92000 | Val: loss = 0.26044, acc = 0.92750
Epoch: 22, att_val: [0.7358748 0.2641258]
Training: loss = 0.23882, acc = 0.92375 | Val: loss = 0.26345, acc = 0.92250
Epoch: 23, att_val: [0.7572328  0.24276905]
Training: loss = 0.23981, acc = 0.92000 | Val: loss = 0.25690, acc = 0.92750
Epoch: 24, att_val: [0.7206972  0.27930224]
Training: loss = 0.22731, acc = 0.91625 | Val: loss = 0.24349, acc = 0.93000
Epoch: 25, att_val: [0.722134  0.2778665]
Training: loss = 0.22202, acc = 0.92625 | Val: loss = 0.23532, acc = 0.92500
Epoch: 26, att_val: [0.7150313 0.2849686]
Training: loss = 0.22268, acc = 0.92875 | Val: loss = 0.23402, acc = 0.92750
Epoch: 27, att_val: [0.7125014  0.28749928]
Training: loss = 0.20761, acc = 0.93625 | Val: loss = 0.23364, acc = 0.92750
Epoch: 28, att_val: [0.69581467 0.30418545]
Training: loss = 0.20781, acc = 0.92875 | Val: loss = 0.23326, acc = 0.92500
Epoch: 29, att_val: [0.71195936 0.28803986]
Training: loss = 0.21449, acc = 0.92250 | Val: loss = 0.23683, acc = 0.92500
Epoch: 30, att_val: [0.71272135 0.28727964]
Training: loss = 0.20843, acc = 0.93750 | Val: loss = 0.24515, acc = 0.92750
Epoch: 31, att_val: [0.7295695  0.27043116]
Training: loss = 0.21223, acc = 0.92625 | Val: loss = 0.24625, acc = 0.93000
Epoch: 32, att_val: [0.7478939 0.2521051]
Training: loss = 0.19831, acc = 0.93750 | Val: loss = 0.25637, acc = 0.92250
Epoch: 33, att_val: [0.765642   0.23435786]
Training: loss = 0.19981, acc = 0.93500 | Val: loss = 0.25155, acc = 0.92250
Epoch: 34, att_val: [0.7662387  0.23376118]
Training: loss = 0.19106, acc = 0.93125 | Val: loss = 0.23686, acc = 0.93000
Epoch: 35, att_val: [0.7615603  0.23844063]
Training: loss = 0.20382, acc = 0.93125 | Val: loss = 0.22409, acc = 0.92500
Epoch: 36, att_val: [0.7838529  0.21614663]
Training: loss = 0.19691, acc = 0.93125 | Val: loss = 0.22044, acc = 0.92500
Epoch: 37, att_val: [0.7693816  0.23061737]
Training: loss = 0.18301, acc = 0.93250 | Val: loss = 0.22209, acc = 0.92500
Epoch: 38, att_val: [0.76936907 0.2306304 ]
Training: loss = 0.19142, acc = 0.93625 | Val: loss = 0.22506, acc = 0.92500
Epoch: 39, att_val: [0.77435964 0.22563985]
Training: loss = 0.19294, acc = 0.93625 | Val: loss = 0.23559, acc = 0.93000
Epoch: 40, att_val: [0.7521367 0.2478629]
Training: loss = 0.18883, acc = 0.94000 | Val: loss = 0.26342, acc = 0.92000
Epoch: 41, att_val: [0.7508665  0.24913341]
Training: loss = 0.17905, acc = 0.93750 | Val: loss = 0.28110, acc = 0.91250
Epoch: 42, att_val: [0.7333757  0.26662552]
Training: loss = 0.21830, acc = 0.92375 | Val: loss = 0.26424, acc = 0.91500
Epoch: 43, att_val: [0.7357207  0.26427943]
Training: loss = 0.19220, acc = 0.92750 | Val: loss = 0.23808, acc = 0.92750
Epoch: 44, att_val: [0.7271987  0.27280077]
Training: loss = 0.17781, acc = 0.94875 | Val: loss = 0.22811, acc = 0.92250
Epoch: 45, att_val: [0.71394783 0.28605238]
Training: loss = 0.16565, acc = 0.94500 | Val: loss = 0.22687, acc = 0.92000
Epoch: 46, att_val: [0.73174715 0.26825106]
Training: loss = 0.17550, acc = 0.94375 | Val: loss = 0.22775, acc = 0.91750
Epoch: 47, att_val: [0.71902454 0.2809751 ]
Training: loss = 0.18009, acc = 0.94125 | Val: loss = 0.23041, acc = 0.92000
Epoch: 48, att_val: [0.7014951  0.29850516]
Training: loss = 0.18962, acc = 0.94000 | Val: loss = 0.23536, acc = 0.92250
Epoch: 49, att_val: [0.6858489  0.31415287]
Training: loss = 0.15952, acc = 0.94500 | Val: loss = 0.24315, acc = 0.92500
load model from : result/dblp/2019-10-15 11:48:30/pre_trained/dblp_allMP_multi_fea_/5/5.ckpt
jhy_final_embedding (4057, 64) 
 [[ 0.13521521 -0.7072619   0.832808   ...  1.112872    1.1787724
  -0.16203792]
 [-0.78494304 -0.34779242  1.472115   ...  1.2154543  -0.23513336
  -0.7456982 ]
 [-0.5363604   1.887032    0.47793886 ...  2.5380664   0.27227885
  -0.39339   ]
 ...
 [-0.7861883  -0.326144    1.4115918  ...  1.1074659  -0.32048142
  -0.75370026]
 [ 0.00994575 -0.66550654  0.8422635  ...  0.45490724 -0.35393614
  -0.37648916]
 [ 0.7031169  -0.8748028   0.09922526 ... -0.8532041  -0.46311286
   0.45393088]]
Test loss: 0.21477437019348145 ; Test accuracy: 0.9275427460670471
start knn, kmean.....
xx: (2857, 64), yy: (2857, 4)
KNN(10avg, split:0.2, k=5) f1_macro: 0.9166, f1_micro: 0.9266
KNN(10avg, split:0.4, k=5) f1_macro: 0.9158, f1_micro: 0.9255
KNN(10avg, split:0.6, k=5) f1_macro: 0.9200, f1_micro: 0.9287
KNN(10avg, split:0.8, k=5) f1_macro: 0.9220, f1_micro: 0.9320
NMI (10 avg): 0.7406 , ARI (10avg): 0.7799
